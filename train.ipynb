{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d643738b",
   "metadata": {},
   "source": [
    "# Fundus Super-Resolution Project\n",
    "\n",
    "This notebook contains two main parts: training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5138e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg19\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import glob\n",
    "import piq\n",
    "\n",
    "# ===============================\n",
    "# CONFIG\n",
    "# ===============================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "HR_DIR   = \"/content/drive/MyDrive/superRes/SR/SRimage\"\n",
    "MASK_DIR = \"/content/drive/MyDrive/superRes/SR/Ground truth\"\n",
    "\n",
    "WORK_DIR = \"/content/work_fundus_sr\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "UPSCALE = 2\n",
    "PATCH_HR = 128\n",
    "VAL_SPLIT = 0.1\n",
    "BATCH_TRAIN = 2\n",
    "BATCH_VAL   = 2\n",
    "ACCUM_STEPS = 2\n",
    "NUM_WORKERS = 2\n",
    "IMG_EXTS = ('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')\n",
    "\n",
    "PERC_RESIZE = 224\n",
    "PERC_DEVICE = 'cpu'\n",
    "\n",
    "EPOCHS = 50\n",
    "EARLY_STOP = 10\n",
    "WARMUP_EPOCHS = 5\n",
    "\n",
    "USE_AMP = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Scale x{UPSCALE} | HRpatch={PATCH_HR} | batch={BATCH_TRAIN} | perc@{PERC_RESIZE} on {PERC_DEVICE}\")\n",
    "\n",
    "# ===============================\n",
    "# MODEL (EDSR Lite)\n",
    "# ===============================\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(c, c, 3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(c, c, 3, padding=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.body(x) * 0.1\n",
    "\n",
    "class EDSR_Lite(nn.Module):\n",
    "    def __init__(self, scale=2, n_res=8, c=64):\n",
    "        super().__init__()\n",
    "        self.head = nn.Conv2d(3, c, 3, padding=1)\n",
    "        self.body = nn.Sequential(*[ResBlock(c) for _ in range(n_res)])\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv2d(c, c * scale * scale, 3, padding=1),\n",
    "            nn.PixelShuffle(scale),\n",
    "            nn.Conv2d(c, 3, 3, padding=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        res = self.body(x)\n",
    "        x = x + res\n",
    "        return self.tail(x)\n",
    "\n",
    "# ===============================\n",
    "# DATASET\n",
    "# ===============================\n",
    "class FundusDataset(Dataset):\n",
    "    def __init__(self, hr_dir, mask_dir, patch_size=128, upscale=2, train=True, split=0.1):\n",
    "        self.hr_files = sorted([f for f in glob.glob(os.path.join(hr_dir, \"*\")) if f.endswith(IMG_EXTS)])\n",
    "        self.mask_files = sorted([f for f in glob.glob(os.path.join(mask_dir, \"*\")) if f.endswith(IMG_EXTS)])\n",
    "        n_val = int(len(self.hr_files) * split)\n",
    "        if train:\n",
    "            self.hr_files = self.hr_files[n_val:]\n",
    "            self.mask_files = self.mask_files[n_val:]\n",
    "        else:\n",
    "            self.hr_files = self.hr_files[:n_val]\n",
    "            self.mask_files = self.mask_files[:n_val]\n",
    "        self.patch_size = patch_size\n",
    "        self.upscale = upscale\n",
    "        self.train = train\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hr_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hr = Image.open(self.hr_files[idx]).convert(\"RGB\")\n",
    "        mask = Image.open(self.mask_files[idx]).convert(\"L\")\n",
    "        hr = self.to_tensor(hr)\n",
    "        mask = self.to_tensor(mask)\n",
    "        lr = F.interpolate(hr.unsqueeze(0), scale_factor=1/self.upscale, mode=\"bicubic\", align_corners=False).squeeze(0)\n",
    "        return lr, hr, mask, os.path.basename(self.hr_files[idx])\n",
    "\n",
    "train_ds = FundusDataset(HR_DIR, MASK_DIR, PATCH_HR, UPSCALE, train=True, split=VAL_SPLIT)\n",
    "val_ds   = FundusDataset(HR_DIR, MASK_DIR, PATCH_HR, UPSCALE, train=False, split=VAL_SPLIT)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_VAL, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ===============================\n",
    "# PERCEPTUAL NETWORK\n",
    "# ===============================\n",
    "class VGGPerceptual(nn.Module):\n",
    "    def __init__(self, resize=True):\n",
    "        super().__init__()\n",
    "        vgg = vgg19(pretrained=True).features[:16].eval()\n",
    "        for p in vgg.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.vgg = vgg\n",
    "        self.resize = resize\n",
    "    def forward(self, x):\n",
    "        if self.resize:\n",
    "            x = F.interpolate(x, size=(PERC_RESIZE,PERC_RESIZE), mode=\"bilinear\", align_corners=False)\n",
    "        return self.vgg(x)\n",
    "\n",
    "perc_net = VGGPerceptual().to(PERC_DEVICE)\n",
    "\n",
    "def perceptual_loss(net, sr, hr):\n",
    "    f1 = net(sr)\n",
    "    f2 = net(hr)\n",
    "    return F.l1_loss(f1, f2)\n",
    "\n",
    "def mask_weighted_l1(sr, hr, mask):\n",
    "    return (torch.abs(sr - hr) * mask).mean()\n",
    "\n",
    "def tv_loss(x):\n",
    "    return torch.mean(torch.abs(x[:, :, :, :-1] - x[:, :, :, 1:])) +            torch.mean(torch.abs(x[:, :, :-1, :] - x[:, :, 1:, :]))\n",
    "\n",
    "ssim_metric = piq.SSIMLoss(data_range=1.).to(device)\n",
    "\n",
    "# ===============================\n",
    "# TRAINING\n",
    "# ===============================\n",
    "model = EDSR_Lite(scale=UPSCALE, n_res=8, c=64).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "BEST_PSNR = -1.0\n",
    "SAVE_PATH = os.path.join(WORK_DIR, \"best_model.pt\")\n",
    "\n",
    "print(\"==> Start Training on\", device, \"(AMP=\", USE_AMP, \")\")\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    tr_loss = 0.0\n",
    "    pbar = tqdm(train_dl, desc=f\"Epoch {ep}/{EPOCHS}\")\n",
    "    for lr_t, hr_t, m_t, _ in pbar:\n",
    "        lr_t, hr_t, m_t = lr_t.to(device), hr_t.to(device), m_t.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            sr = model(lr_t).clamp(0,1)\n",
    "            l1 = mask_weighted_l1(sr, hr_t, m_t)\n",
    "            perc = perceptual_loss(perc_net, sr.to(PERC_DEVICE).float(), hr_t.to(PERC_DEVICE).float())\n",
    "            ssim_l = 1.0 - ssim_metric(sr.float(), hr_t.float())\n",
    "            tv = tv_loss(sr)\n",
    "            loss = 0.6*l1 + 0.3*perc + 0.1*ssim_l + 0.05*tv\n",
    "        scaler.scale(loss).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        tr_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    v_psnr, v_ssim, v_loss, n = 0.,0.,0.,0\n",
    "    with torch.no_grad():\n",
    "        for lr_t, hr_t, m_t, _ in val_dl:\n",
    "            lr_t, hr_t, m_t = lr_t.to(device), hr_t.to(device), m_t.to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "                sr = model(lr_t).clamp(0,1)\n",
    "                l1 = mask_weighted_l1(sr, hr_t, m_t)\n",
    "                perc = perceptual_loss(perc_net, sr.to(PERC_DEVICE).float(), hr_t.to(PERC_DEVICE).float())\n",
    "                ssim_l = 1.0 - ssim_metric(sr.float(), hr_t.float())\n",
    "                tv = tv_loss(sr)\n",
    "                loss = 0.6*l1 + 0.3*perc + 0.1*ssim_l + 0.05*tv\n",
    "            v_loss += loss.item()\n",
    "            v_psnr += piq.psnr(sr.float(), hr_t.float(), data_range=1.).item()\n",
    "            v_ssim += 1.0 - ssim_l.item()\n",
    "            n += 1\n",
    "    v_loss/=n; v_psnr/=n; v_ssim/=n\n",
    "    print(f\"[VAL] loss={v_loss:.4f} | PSNR={v_psnr:.2f} | SSIM={v_ssim:.3f}\")\n",
    "    if v_psnr > BEST_PSNR:\n",
    "        BEST_PSNR = v_psnr\n",
    "        torch.save({\"model\": model.state_dict(), \"cfg\": {\"scale\": UPSCALE}}, SAVE_PATH)\n",
    "        print(f\"  Saved best @ PSNR {BEST_PSNR:.2f}\")\n",
    "\n",
    "# Save final model\n",
    "FINAL_PATH = os.path.join(WORK_DIR, \"final_model.pt\")\n",
    "torch.save({\"model\": model.state_dict(), \"cfg\": {\"scale\": UPSCALE}}, FINAL_PATH)\n",
    "print(\"âœ… Model saved to:\", FINAL_PATH)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
